{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.num_parameters()) # 427616513 params for large-patch14\n",
    "                              # 151277313 params for base-patch32\n",
    "                              # 149620737 params for base-patch16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # boilerplate code from huggingface docs\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# plt.imshow(image)\n",
    "# inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "# print(outputs.text_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "caption_path = \"/Users/echen/Desktop/CSE_6242.nosync/CSE6242-Columnists/data_vis/summaries\"\n",
    "cartoon_path = \"/Users/echen/Desktop/CSE_6242.nosync/CSE6242-Columnists/data_vis/cartoons\"\n",
    "ids = set()\n",
    "cap_paths = []\n",
    "img_paths = []\n",
    "for file in os.listdir(caption_path):\n",
    "    file_path = os.path.join(caption_path, file)\n",
    "    if not os.path.isfile(file_path) or file[-3:]!='csv' or file[:3] in ids:\n",
    "        continue\n",
    "    ids.add(file[:3])\n",
    "    cap_paths.append(file_path)\n",
    "for file in os.listdir(cartoon_path):\n",
    "    file_path = os.path.join(cartoon_path, file)\n",
    "    if not os.path.isfile(file_path) or file[-3:]!='jpg':\n",
    "        continue\n",
    "    if file[:3] not in ids: \n",
    "        print(\"extra_img: \", file)\n",
    "        continue\n",
    "    img_paths.append(file_path)\n",
    "id_list = list(ids)\n",
    "id_list.sort()\n",
    "cap_paths.sort()\n",
    "img_paths.sort()\n",
    "assert(len(img_paths) == len(cap_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=50)\n",
    "# condensed_data = pca.fit_transform(img1_cap)\n",
    "# print(condensed_data.shape, np.max(condensed_data), np.min(condensed_data))\n",
    "# tsne = TSNE()\n",
    "# tsne_embeds = tsne.fit_transform(condensed_data)\n",
    "# norm_tsne_embeds = np.zeros(tsne_embeds.shape)\n",
    "# mins = np.min(tsne_embeds, axis=0, keepdims=True)\n",
    "# maxes = np.max(tsne_embeds, axis=0, keepdims=True)\n",
    "# norm_tsne_embeds = (tsne_embeds-mins)/(maxes-mins)\n",
    "# print(norm_tsne_embeds.shape, np.max(norm_tsne_embeds), np.min(norm_tsne_embeds))\n",
    "# print(norm_tsne_embeds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(model, image, caption_list):\n",
    "    inputs = processor(text=caption_list, images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    text_embeds = outputs.text_embeds.detach().numpy()\n",
    "    img_embeds = outputs.image_embeds.detach().numpy()\n",
    "    return text_embeds, img_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "tsne = TSNE()\n",
    "def get_tsne_embeds(img_df):\n",
    "    caps = img_df[\"cap_feat\"]\n",
    "    cap_data = np.vstack(caps)\n",
    "    condensed_data = pca.fit_transform(cap_data)\n",
    "    tsne_embeds = tsne.fit_transform(condensed_data)\n",
    "    norm_tsne_embeds = np.zeros(tsne_embeds.shape)\n",
    "    mins = np.min(tsne_embeds, axis=0, keepdims=True)\n",
    "    maxes = np.max(tsne_embeds, axis=0, keepdims=True)\n",
    "    norm_tsne_embeds = (tsne_embeds-mins)/(maxes-mins)\n",
    "    norm_tsne_embeds_df = pd.DataFrame({'X': norm_tsne_embeds[:,0], 'Y': norm_tsne_embeds[:,1], \"caption\": img_df[\"caption\"]})\n",
    "    return norm_tsne_embeds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_captions(cap_df, filter_len=50):\n",
    "    cap_df[\"sentence_len\"] = cap_df[\"caption\"].apply(lambda x: len(x.split()))\n",
    "    return cap_df[cap_df[\"sentence_len\"] <=filter_len].reset_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "def get_embeds_from_path(img_id, img_path, cap_path, batch_size=32):\n",
    "    df_list = []\n",
    "    img = Image.open(img_path)\n",
    "    inputs = processor(text=['test'], images=img, return_tensors=\"pt\", padding=True)\n",
    "    img_feat = model(**inputs).image_embeds.detach().numpy()\n",
    "    cap_csv = pd.read_csv(cap_path)\n",
    "    cap_csv = filter_long_captions(cap_csv)\n",
    "    cap_csv[\"img_id\"] = img_id\n",
    "    cap_csv[\"cap_feat\"] = None\n",
    "    cap_csv[\"img_feat\"] = [img_feat]*len(cap_csv.index)\n",
    "    caption_list = []\n",
    "    for idx,row in cap_csv.iterrows():\n",
    "        caption_list.append(row['caption'])\n",
    "        if (idx == (len(cap_csv.index)-1)):\n",
    "            text_embeds, _ = get_embeds(model, img, caption_list)\n",
    "            for i,embed in enumerate(text_embeds):\n",
    "                cap_csv.at[idx-len(text_embeds)+1+i,\"cap_feat\"] = text_embeds[i]\n",
    "            caption_list = []\n",
    "        if (len(caption_list) != batch_size):continue\n",
    "        text_embeds, _ = get_embeds(model, img, caption_list)\n",
    "        for i,embed in enumerate(text_embeds):\n",
    "            cap_csv.at[idx-batch_size+1+i,\"cap_feat\"] = text_embeds[i]\n",
    "        caption_list = []\n",
    "    df_list.append(cap_csv)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\"caption\", \"img_id\", \"mean\",\"votes\", \"img_feat\", \"cap_feat\"]\n",
    "def process_dataset_tsne(id_list, img_paths, cap_paths):\n",
    "    tsne_list = []\n",
    "    cap_list = []\n",
    "    count = 0\n",
    "    for img_id, img_path, cap_path in zip(id_list, img_paths, cap_paths):\n",
    "        # process for TSNE data\n",
    "\n",
    "        img_df = get_embeds_from_path(img_id, img_path, cap_path)\n",
    "        tsne_feats = get_tsne_embeds(img_df)\n",
    "        tsne_feats[\"img_id\"] = img_id\n",
    "        tsne_list.append(tsne_feats)\n",
    "\n",
    "        # save complete dataset\n",
    "        cap_csv = pd.read_csv(cap_path)\n",
    "        cap_csv[\"img_id\"] = img_id\n",
    "        cap_csv[\"img_feat\"] = img_df[\"img_feat\"]\n",
    "        cap_csv[\"cap_feat\"] = img_df[\"cap_feat\"]\n",
    "        cap_list.append(cap_csv[relevant_columns])\n",
    "        break\n",
    "        # if count >= 1:break\n",
    "        # count+=1\n",
    "    return pd.concat(tsne_list), pd.concat(cap_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create complete TSNE DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_df = process_dataset_tsne(id_list, img_paths, cap_paths)\n",
    "tsne_df, condensed_cap_df = process_dataset_tsne(id_list, img_paths, cap_paths)\n",
    "tsne_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of preprocessing, we will likely want to save the original dataset dataframes without the precison or score breakdown columns. The TSNE dataframe above can be saved as is since it should be possible to filter for the correct img_id when necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condensed_cap_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building FF Scoring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Combine the embeddings into a single feature array\n",
    "# Note: This assumes 'cap_feat' and 'img_feat' are each a list or array of 512 floats.\n",
    "X = np.hstack([np.vstack(df['img_feat'].values), np.vstack(df['cap_feat'].values)])\n",
    "\n",
    "# Your target variable\n",
    "y = df['mean'].values/2\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumorRatingNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HumorRatingNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512) # 1024 inputs (512 from image + 512 from caption), to 512 outputs\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)  # 512 inputs to 256 outputs\n",
    "        self.fc3 = nn.Linear(256, 1)    # 256 inputs to 1 output (your mean humor rating)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation here, as we're predicting a continuous value\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "test_data = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = HumorRatingNN()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewYorker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
