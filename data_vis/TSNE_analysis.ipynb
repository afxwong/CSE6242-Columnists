{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/echen/miniforge3/envs/DVA/lib/python3.12/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151277313\n"
     ]
    }
   ],
   "source": [
    "print(model.num_parameters()) # 427616513 params for large-patch14\n",
    "                              # 151277313 params for base-patch32\n",
    "                              # 149620737 params for base-patch16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # boilerplate code from huggingface docs\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# plt.imshow(image)\n",
    "# inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "# print(outputs.text_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra_img:  890.jpg\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "caption_path = \"/Users/echen/Desktop/CSE_6242.nosync/caption-contest-data/summaries\"\n",
    "cartoon_path = \"/Users/echen/Desktop/CSE_6242.nosync/caption-contest-data/cartoons\"\n",
    "ids = set()\n",
    "cap_paths = []\n",
    "img_paths = []\n",
    "for file in os.listdir(caption_path):\n",
    "    file_path = os.path.join(caption_path, file)\n",
    "    if not os.path.isfile(file_path) or file[-3:]!='csv' or file[:3] in ids:\n",
    "        continue\n",
    "    ids.add(file[:3])\n",
    "    cap_paths.append(file_path)\n",
    "for file in os.listdir(cartoon_path):\n",
    "    file_path = os.path.join(cartoon_path, file)\n",
    "    if not os.path.isfile(file_path) or file[-3:]!='jpg':\n",
    "        continue\n",
    "    if file[:3] not in ids: \n",
    "        print(\"extra_img: \", file)\n",
    "        continue\n",
    "    img_paths.append(file_path)\n",
    "id_list = list(ids)\n",
    "id_list.sort()\n",
    "cap_paths.sort()\n",
    "img_paths.sort()\n",
    "assert(len(img_paths) == len(cap_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeds(model, image, caption_list):\n",
    "    inputs = processor(text=caption_list, images=img, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    text_embeds = outputs.text_embeds.detach().numpy()\n",
    "    img_embeds = outputs.image_embeds.detach().numpy()\n",
    "    return text_embeds, img_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      rank                                            caption      mean  \\\n",
      "0        0          I'm a congressman--obstruction is my job.  1.913043   \n",
      "1        1  I'm what they mean when they say, 'The middle ...  1.842105   \n",
      "2        2                  Does this suit make me look flat?  1.711111   \n",
      "3        3    When the right woman comes along, I'll know it.  1.625000   \n",
      "4        4  I used to lie in the gutter, but then I quit d...  1.617647   \n",
      "...    ...                                                ...       ...   \n",
      "3900  3900  Just getting material for my book, \" Humanity ...  1.000000   \n",
      "3901  3901     This has 'Alice in Wonderland' beat by a mile.  1.000000   \n",
      "3902  3902                    I could use a quick pick-me-up.  1.000000   \n",
      "3903  3903     This is the secret to attracting foot traffic.  1.000000   \n",
      "3904  3904                 An aloof view of clouds colliding.  1.000000   \n",
      "\n",
      "      precision  votes  not_funny  somewhat_funny  funny img_id  \\\n",
      "0      0.094022     69         24              27     18    510   \n",
      "1      0.191381     19          8               6      5    510   \n",
      "2      0.112915     45         21              16      8    510   \n",
      "3      0.116657     32         15              14      3    510   \n",
      "4      0.133610     34         19               9      6    510   \n",
      "...         ...    ...        ...             ...    ...    ...   \n",
      "3900   0.117851      9          9               0      0    510   \n",
      "3901   0.117851      9          9               0      0    510   \n",
      "3902   0.117851      9          9               0      0    510   \n",
      "3903   0.117851      9          9               0      0    510   \n",
      "3904   0.117851      9          9               0      0    510   \n",
      "\n",
      "                                               cap_feat  \\\n",
      "0     [-0.0024316842, 0.015905969, 0.011368003, 0.01...   \n",
      "1     [0.013644679, -0.013433615, 0.018261917, -0.00...   \n",
      "2     [0.013936422, 0.0062422995, -0.026340615, 0.00...   \n",
      "3     [0.024949286, 0.017259136, 0.012959481, -0.016...   \n",
      "4     [0.017066555, 0.027702762, -0.015552031, -0.02...   \n",
      "...                                                 ...   \n",
      "3900  [0.012033111, -0.015283988, 0.0011412074, 0.00...   \n",
      "3901  [-0.013423467, 0.036359485, 0.0142259905, 0.02...   \n",
      "3902  [0.014474391, -0.001932845, -0.017554257, -0.0...   \n",
      "3903  [0.0030683381, -0.024004618, 0.014991271, -0.0...   \n",
      "3904                                               None   \n",
      "\n",
      "                                               img_feat  \n",
      "0     [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "1     [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "2     [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "3     [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "4     [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "...                                                 ...  \n",
      "3900  [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "3901  [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "3902  [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "3903  [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "3904  [[0.009510918, 0.024594594, 0.007838643, -0.02...  \n",
      "\n",
      "[3905 rows x 11 columns]\n",
      "379\n"
     ]
    }
   ],
   "source": [
    "columns = [\"img_id\", \"img_feat\", \"cap_feat\", \"mean\",\"precision\", \"votes\", \"not_funny\", \"somewhat_funny\", \"funny\"]\n",
    "new_cols = [\"img_id\", \"img_feat\", \"cap_feat\"]\n",
    "dataset = pd.DataFrame(columns=columns)\n",
    "batch_size = 32\n",
    "df_list = []\n",
    "for img_id, img_path, cap_path in zip(id_list, img_paths, cap_paths):\n",
    "    img = Image.open(img_path)\n",
    "    inputs = processor(text=['test'], images=img, return_tensors=\"pt\", padding=True)\n",
    "    img_feat = model(**inputs).image_embeds.detach().numpy()\n",
    "    cap_csv = pd.read_csv(cap_path)\n",
    "    cap_csv[\"img_id\"] = img_id\n",
    "    cap_csv[\"cap_feat\"] = None\n",
    "    cap_csv[\"img_feat\"] = [img_feat]*len(cap_csv.index)\n",
    "    caption_list = []\n",
    "    for idx,row in cap_csv.iterrows():\n",
    "        caption_list.append(row['caption'])\n",
    "        if (len(caption_list) != batch_size):continue\n",
    "        text_embeds, _ = get_embeds(model, img, caption_list)\n",
    "        for i,embed in enumerate(text_embeds):\n",
    "            cap_csv.at[idx-batch_size+1+i,\"cap_feat\"] = text_embeds[i]\n",
    "        caption_list = []\n",
    "    # print(len(caption_list))\n",
    "    if (len(caption_list) > 0):\n",
    "        text_embeds, _ = get_embeds(model, img, caption_list)\n",
    "        for i,embed in enumerate(text_embeds):\n",
    "            cap_csv.at[idx-batch_size+1+i,\"cap_feat\"] = text_embeds[i]\n",
    "    df_list.append(cap_csv)\n",
    "    print(pd.concat(df_list))\n",
    "    break\n",
    "print(len(ids)) #889-510, 525 does not exist in dataset for some reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewYorker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
